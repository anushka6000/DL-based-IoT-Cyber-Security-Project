{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disagreement based SSL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOf8pgYlCzrF",
        "outputId": "c76c527d-d1a6-487c-c507-94c2d3c1c6de"
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/gdrive/')\n",
        "sys.path.append('/content/gdrive/My Drive/IOT_DL_project/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-GaakxhEQ7i",
        "outputId": "06da55c1-0bf9-434a-a7e8-b8e9258a0576"
      },
      "source": [
        "%cd '/content/gdrive/My Drive/IOT_DL_project/dataset/1.Bot-IoT/train_test_only_five_percent/All_features'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1iU-5odoNw4f2D_AgPNxVDuu6YX9juYYv/IOT_DL_project/dataset/1.Bot-IoT/train_test_only_five_percent/All_features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HepLOSHEXjt"
      },
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.utils import to_categorical "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq4w-StN9ifz"
      },
      "source": [
        "### Attack vs Non-Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nrVn9ZcUVSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d517f8-9dc5-48d2-bbb9-1718dd211335"
      },
      "source": [
        "df=pd.read_csv(\"UNSW_2018_IoT_Botnet_Full5pc_4.csv\")\n",
        "cols=list(df.columns.values)\n",
        "\n",
        "arr=pd.unique(df['state'])\n",
        "dict={'RST':1, 'CON':2, 'REQ':3, 'INT':4, 'URP':5}\n",
        "df['state']=df['state'].map(dict)\n",
        "arr2=pd.unique(df['flgs'])\n",
        "arr3=pd.unique(df['proto'])\n",
        "dict2={'e':1, 'e s':2, 'e d':3, 'e *':4, 'e g':5, 'eU':6}\n",
        "df['flgs']=df['flgs'].map(dict2)\n",
        "dict3={'tcp':1, 'arp':2, 'udp':3, 'icmp':4}\n",
        "df['proto']=df['proto'].map(dict3)\n",
        "\n",
        "\n",
        "df.drop('stime',inplace=True,axis=1)\n",
        "df.drop('ltime',inplace=True,axis=1)\n",
        "df.drop('pkSeqID',inplace=True,axis=1)\n",
        "df.drop('category',inplace=True,axis=1)\n",
        "df.drop('subcategory',inplace=True,axis=1)\n",
        "df.drop('dport',inplace=True,axis=1)\n",
        "df.drop('sport',inplace=True,axis=1)\n",
        "\n",
        "arr4=pd.unique(df['saddr'])\n",
        "dict4={'192.168.100.147':1, '192.168.100.7':2, '192.168.100.148':3, '192.168.100.6':4,\n",
        " '192.168.100.149':5, '192.168.100.150':6, '192.168.100.5':7, '192.168.100.3':8}\n",
        "df['saddr']=df['saddr'].map(dict4)\n",
        "df['daddr']=df['daddr'].map(dict4)\n",
        "\n",
        "df=df.dropna()\n",
        "\n",
        "x,y=df.values[:,:-1],df.values[:,-1:]\n",
        "x=StandardScaler().fit_transform(x)\n",
        "\n",
        "sm=SMOTE(k_neighbors=6)\n",
        "x,y=sm.fit_resample(x,y)\n",
        "\n",
        "x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.2,random_state=12)\n",
        "x_val,x_test,y_val,y_test=train_test_split(x_val,y_val,test_size=0.5,random_state=12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (7,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzl2NGudE1fU",
        "outputId": "9adc83dd-b280-4e48-b2f8-bcfb4046131e"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1058508, 38)\n",
            "(132314, 38)\n",
            "(1058508,)\n",
            "(132314,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVO77zNDGCXL"
      },
      "source": [
        "class TriTrainingwDisagreement():\n",
        "\n",
        "    def __init__(self, classifier):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            classifier - classifier, with .fit, .predict API (refer to classifiers of sklearn)\n",
        "        \"\"\"\n",
        "        # Initialize\n",
        "        if sklearn.base.is_classifier(classifier):\n",
        "            self.clf = [sklearn.base.clone(classifier) for i in range(3)]\n",
        "        else:\n",
        "            self.clf = [sklearn.base.clone(classifier[i]) for i in range(3)]\n",
        "\n",
        "    def measure_error(self, j, k):\n",
        "        \"\"\"\n",
        "        args:\n",
        "                j - int, classifier index\n",
        "                k - int, classifier index\n",
        "        return:\n",
        "                float, classification_error\n",
        "        \"\"\"\n",
        "        y_predict_j = self.clf[j].predict(self.X_label)\n",
        "        y_predict_k = self.clf[k].predict(self.X_label)\n",
        "        return (1 - np.sum((y_predict_j == y_predict_k) & (y_predict_j == self.y_label)) / np.sum(y_predict_j == y_predict_k))\n",
        "\n",
        "    def fit(self, X_label, y_label, X_unlabel):\n",
        "        \"\"\"\n",
        "        args:\n",
        "                X_label - labeled train feature vector (ndarray of size, # of samples * # of features), features are numeric numbers\n",
        "                y_label - labeled train label vector (ndarray of size, # of samples), labels are numeric numbers\n",
        "                X_unlabel - test feature vector (ndarray of size, # of samples * # of features), features are numeric numbers\n",
        "        \"\"\"        \n",
        "\n",
        "        self.X_label = X_label\n",
        "        self.y_label = y_label\n",
        "\n",
        "        classification_error_current = [0.5, 0.5, 0.5]\n",
        "        classification_error = [0.5, 0.5, 0.5]\n",
        "        pseudo_label_size_current = [0, 0, 0]\n",
        "        pseudo_label_size = [0, 0, 0]\n",
        "        # pseudo_label_index used to compare and check if tri-training can be stopped, when two iterations have the same label_index, means tri-training can be stopped\n",
        "        X_pseudo_label_index = [[], [], []]\n",
        "        X_pseudo_label_index_current = [[], [], []]\n",
        "\n",
        "        feature_size = self.X_label.shape[1]\n",
        "\n",
        "        # Train each classifier with bootstrampped subset\n",
        "        for i in range(3):\n",
        "            X_resample, y_resample = sklearn.utils.resample(self.X_label, self.y_label)  # BootstrapSample(L)\n",
        "            self.clf[i].fit(X_resample, y_resample)  # Learn(Si)\n",
        "\n",
        "        iteration = 0\n",
        "        while (True):\n",
        "\n",
        "            update = [False, False, False]\n",
        "\n",
        "            iteration = iteration + 1\n",
        "            for i in range(3):\n",
        "                X_pseudo_label_index_current[i] = X_pseudo_label_index[i]\n",
        "\n",
        "            # Step3.1 Set Li = empty set, Li denotes the new pseudo label set determined by tri-training iteration for classifier i\n",
        "            # X_pseudo_label_index, contains the data record index (in the full unlabelled set) of the new pseudo label set determined by tri-training iteration for classifier i\n",
        "            # X_pseudo_label, contains the features for new pseudo label set determined by tri-training iteration for classifier i\n",
        "            # y_pseudo_label, contains the labels (not ground truth label, but pseudo label calculated by tri-training iteration) for new pseudo label set determined by tri-training iteration for classifier i\n",
        "            X_pseudo_label_index = [[], [], []]\n",
        "            X_pseudo_label = [[], [], []]\n",
        "            y_pseudo_label = [[], [], []]\n",
        "\n",
        "            # Step 3.2 Loop through all the data record in unlabelled set\n",
        "            for i in range(3):\n",
        "                j, k = np.delete(np.array([0, 1, 2]), i)\n",
        "                classification_error[i] = self.measure_error(j, k)\n",
        "                if classification_error[i] < classification_error_current[i]:\n",
        "                    # Step 3.2 If classifier j,k aggrees with the label for one data record, and not agree with classifier i, in unlabelled set,\n",
        "                    # then add the data record into Li                    \n",
        "                    y_predict_j = self.clf[j].predict(X_unlabel)\n",
        "                    y_predict_k = self.clf[k].predict(X_unlabel)\n",
        "                    y_predict_i = self.clf[i].predict(X_unlabel)\n",
        "                    y_pseudo_label[i] = y_predict_j[np.logical_and(y_predict_j==y_predict_k,y_predict_j!=y_predict_i)]\n",
        "                    X_pseudo_label_index[i] = np.where(np.logical_and(y_predict_j==y_predict_k,y_predict_j!=y_predict_i))\n",
        "                    \n",
        "                    pseudo_label_size[i] = len(X_pseudo_label_index[i])\n",
        "                    #print(\"classification_error: {}, classification_error_current: {}, pseudo_label_size: {}, pseudo_label_size_current: {}\".format(classification_error[i], classification_error_current[i], pseudo_label_size[i],pseudo_label_size_current[i]))\n",
        "\n",
        "                    if pseudo_label_size_current[i] == 0:\n",
        "                        pseudo_label_size_current[i] = math.floor(classification_error[i] / (classification_error_current[i] - classification_error[i]) + 1)\n",
        "                    if pseudo_label_size_current[i] < pseudo_label_size[i]:\n",
        "                        if ((classification_error[i] * pseudo_label_size[i]) < (classification_error_current[i] * pseudo_label_size_current[i])):\n",
        "                            update[i] = True\n",
        "                        elif pseudo_label_size_current[i] > (classification_error[i] / (classification_error_current[i] - classification_error[i])):\n",
        "                            resample_size = math.ceil(classification_error_current[i] * pseudo_label_size_current[i] / classification_error[i] - 1)\n",
        "                            X_pseudo_label_index[i], y_pseudo_label[i] = sklearn.utils.resample(X_pseudo_label_index[i],y_pseudo_label[i],replace=False,n_samples=resample_size)\n",
        "                            pseudo_label_size[i] = len(X_pseudo_label_index[i])\n",
        "                            update[i] = True\n",
        "\n",
        "            # Step 3.3 Train all the three classifiers with Li + original labelled data set\n",
        "            for i in range(3):\n",
        "                if update[i] == True:\n",
        "                    #print(\"number of pseudo labels added for classifier {} is: {}\".format(i,len(X_pseudo_label_index[i])))\n",
        "                    X_pseudo_label[i] = np.array(X_unlabel[X_pseudo_label_index[i]])\n",
        "                    self.clf[i].fit(np.concatenate((X_pseudo_label[i], self.X_label), axis=0),np.concatenate((np.array(y_pseudo_label[i]), self.y_label), axis=0))\n",
        "                    classification_error_current[i] = classification_error[i]\n",
        "                    pseudo_label_size_current[i] = pseudo_label_size[i]\n",
        "\n",
        "            # Stop tri-training process, if the pseudo label data set added in current tri-training iteration\n",
        "            # is the same for last tri-training iteration for all classifiers\n",
        "            if (np.array_equal(X_pseudo_label_index[0], X_pseudo_label_index_current[0]) & np.array_equal(X_pseudo_label_index[1], X_pseudo_label_index_current[1]) \n",
        "                    & np.array_equal(X_pseudo_label_index[2], X_pseudo_label_index_current[2])):\n",
        "                break\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        \"\"\"\n",
        "        args:\n",
        "                X_test - test feature vector (ndarray of size, # of samples * # of features), features are numeric numbers\n",
        "        return:\n",
        "                array of size (# of test samples), with values as predicted label 1 or 0\n",
        "        \"\"\"\n",
        "        I = self.clf[0].predict(X_test)\n",
        "        J = self.clf[1].predict(X_test)\n",
        "        K = self.clf[2].predict(X_test)\n",
        "        I[J == K] = J[J == K]\n",
        "        return I\n",
        "\n",
        "    def score(self, X_test, y_test):\n",
        "        \"\"\" \n",
        "        args:\n",
        "                X_test - test feature vector (ndarray of size, # of samples * # of features), features are numeric numbers\n",
        "                y_test - test label vector (ndarray of size, # of samples), labels are numeric numbers\n",
        "        return:\n",
        "                float, accuracy_score of predicted value by the tri-training (with disagreement) classifier against groud truth\n",
        "        \"\"\"\n",
        "        \n",
        "        return sklearn.metrics.accuracy_score(y_test, self.predict(x_test))\n",
        "    def prec(self, X_test, y_test):\n",
        "        \"\"\" \n",
        "        args:\n",
        "                X_test - test feature vector (ndarray of size, # of samples * # of features), features are numeric numbers\n",
        "                y_test - test label vector (ndarray of size, # of samples), labels are numeric numbers\n",
        "        return:\n",
        "                float, accuracy_score of predicted value by the tri-training (with disagreement) classifier against groud truth\n",
        "        \"\"\"\n",
        "        \n",
        "        return sklearn.metrics.precision_score(y_test, self.predict(x_test))\n",
        "    def recall(self, X_test, y_test):\n",
        "        return sklearn.metrics.recall_score(y_test, self.predict(x_test))\n",
        "    def F1(self, X_test, y_test):\n",
        "        return sklearn.metrics.f1_score(y_test, self.predict(x_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPcwfnYoKvAZ"
      },
      "source": [
        "def data_process(X_train, y_train, rate):\n",
        "  rng = np.random.RandomState(0)#to make same index every time\n",
        "  labeled_index = rng.rand(len(y_train)) < rate#in training set, choose 20% as labeled data\n",
        "  unlabeled_index = np.logical_not(labeled_index)\n",
        "  L_data = X_train[labeled_index]#data of L\n",
        "  L_label = y_train[labeled_index]#lable of L\n",
        "  U_data = X_train[unlabeled_index]#data of U\n",
        "  return L_data, L_label, U_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWKWR__BL7u5"
      },
      "source": [
        "classifier = {}\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import tree\n",
        "from sklearn import linear_model\n",
        "from sklearn import svm\n",
        "\n",
        "classifier['DecisionTree'] = tree.DecisionTreeClassifier()\n",
        "classifier['BP_Network'] = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(20, 10), random_state=1)\n",
        "classifier['NaiveBayes'] = GaussianNB()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L_data, L_label, U_data, X_test, y_test = data_process(x,y, 0.25)\n",
        "\n",
        "classifiers = [sklearn.base.clone(classifier[c])]\n",
        "for clf in classifier.keys():\n",
        "    if clf != c:\n",
        "        print(clf)\n",
        "        classifiers.append(sklearn.base.clone(classifier[clf]))#use the first clf in classifiers to output score, but all three to label data\n",
        "print(classifiers)\n",
        "m = TriTraining(classifiers)\n",
        "1 - m.train(L_data, L_label, U_data, X_test, y_test)"
      ],
      "metadata": {
        "id": "sqEiqi69Hwkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25tiKyhQNXmb",
        "outputId": "40b0ac6c-6404-4114-be31-2884b710dba6"
      },
      "source": [
        "for c in classifier:\n",
        "        for r in [0.2]:        \n",
        "            \n",
        "            print('classifier:', c)\n",
        "            print('label_rate:', r)\n",
        "            error = np.zeros([1,20])\n",
        "            accuracy = np.zeros([1,20])\n",
        "            precision = np.zeros([1,20])\n",
        "            recall = np.zeros([1,20])\n",
        "            f1 = np.zeros([1,20])\n",
        "            time_model = np.zeros([1,20])\n",
        "            for i in range(20):#average on 20 data splits\n",
        "                L_data, L_label, U_data = data_process(x_train, y_train, r)\n",
        "               \n",
        "                m2 = TriTrainingwDisagreement(classifier[c])\n",
        "                start = time.time()\n",
        "                m2.fit(L_data, L_label, U_data)\n",
        "                stop = time.time()\n",
        "                error[0, i] = 1-m2.score(x_test, y_test)\n",
        "                accuracy[0,i]=m2.score(x_test,y_test)\n",
        "                precision[0,i]=m2.prec(x_test,y_test)\n",
        "                recall[0,i]=m2.recall(x_test,y_test)\n",
        "                f1[0,i]=m2.F1(x_test,y_test)\n",
        "                time_model[0,i]= stop-start\n",
        "\n",
        "               \n",
        "                \n",
        "            e = np.mean(error, axis=1)\n",
        "            a = np.mean(accuracy, axis=1)\n",
        "            p = np.mean(precision, axis=1)\n",
        "            rec = np.mean(recall, axis=1)\n",
        "            f = np.mean(f1, axis=1)\n",
        "            print('TriTraining Disagree test error=', e)\n",
        "            print('TriTraining Disagree test accuracy=', a)\n",
        "            print('TriTraining Disagree test precision=', p)\n",
        "            print('TriTraining Disagree test recall score=', rec)\n",
        "            print('TriTraining Disagree test F1 score=', f)\n",
        "            print('Training time=', np.mean(time_model, axis=1))\n",
        "            print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier: DecisionTree\n",
            "label_rate: 0.2\n",
            "TriTraining Disagree test error= [0.]\n",
            "TriTraining Disagree test accuracy= [1.]\n",
            "TriTraining Disagree test precision= [1.]\n",
            "TriTraining Disagree test recall score= [1.]\n",
            "TriTraining Disagree test F1 score= [1.]\n",
            "Training time= [4.35212173]\n",
            "\n",
            "\n",
            "classifier: BP_Network\n",
            "label_rate: 0.2\n",
            "TriTraining Disagree test error= [0.19233792]\n",
            "TriTraining Disagree test accuracy= [0.80766208]\n",
            "TriTraining Disagree test precision= [1.]\n",
            "TriTraining Disagree test recall score= [0.61509725]\n",
            "TriTraining Disagree test F1 score= [0.76168447]\n",
            "Training time= [10.53135269]\n",
            "\n",
            "\n",
            "classifier: NaiveBayes\n",
            "label_rate: 0.2\n",
            "TriTraining Disagree test error= [3.77888961e-07]\n",
            "TriTraining Disagree test accuracy= [0.99999962]\n",
            "TriTraining Disagree test precision= [1.]\n",
            "TriTraining Disagree test recall score= [0.99999924]\n",
            "TriTraining Disagree test F1 score= [0.99999962]\n",
            "Training time= [11.38365376]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}